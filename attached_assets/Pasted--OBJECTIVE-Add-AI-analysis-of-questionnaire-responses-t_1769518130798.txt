[OBJECTIVE]
Add AI analysis of questionnaire responses that adapts its voice and focus based 
on the selected persona. Integrates with the existing test recording form from Prompt 5.

[CONTEXT]
After Prompt 6:
- Questionnaire tab displays persona-filtered questions
- Users can record responses to questions
- selected_persona stored per control
- Responses stored in implementation_responses JSONB

The test recording form from Prompt 5 exists at `/controls/:controlNumber/test`.
We need to add AI analysis capability that is persona-aware.

[DO NOT REBUILD - ENHANCE EXISTING]

Keep unchanged from Prompt 5:
- Test recording form structure and layout
- Status dropdown with all 6 options (Pass, Pass (Previous), Fail, Blocked, Not Attempted, Continual Improvement)
- Comments textarea
- Cancel and Submit buttons
- Navigation and routing
- Immutable INSERT-only behavior for test_runs

Only ADD these new elements:
- "Analyze with AI" button
- Streaming analysis display panel
- AI suggested status with confidence
- Persona indicator showing which persona is being used

[PERSONA BEHAVIOR PROFILES]

### Auditor Persona
**Voice:** Formal, evidence-focused, verification-oriented
**System prompt emphasis:**
- Check responses against `red_flags` - explicitly flag any matches
- Evaluate using `auditor_focus` criteria from each question
- Reference `nc_pattern` when issues found
- Be specific about evidence gaps per `evidence_type`
- Frame findings as audit observations

**Output emphasis:**
- Red flags triggered (list)
- Non-conformity risk areas
- Evidence gaps identified
- Compliance/non-compliance determination

### Advisor Persona  
**Voice:** Collaborative, improvement-focused, consultative
**System prompt emphasis:**
- Compare responses to `what_good_looks_like` criteria
- Identify improvement opportunities constructively
- Reference `related_controls` for holistic recommendations
- Focus on maturity progression and best practices
- Balance critique with recognition of strengths

**Output emphasis:**
- Improvement opportunities (prioritized list)
- Maturity assessment (Initial/Developing/Defined/Managed/Optimized)
- Related controls to review
- Best practice recommendations

### Analyst Persona
**Voice:** Data-driven, objective, metrics-focused
**System prompt emphasis:**
- Quantify compliance level as percentage/score
- Count criteria met vs. total criteria per question
- Identify measurable gaps
- Suggest KPIs for ongoing monitoring
- Present statistical summary

**Output emphasis:**
- Compliance score (0-100%)
- Criteria breakdown (X of Y met)
- Gap quantification
- Suggested monitoring KPIs

[REQUIREMENTS]

### 1. Enhanced Test Recording Form

Add to existing form (above Submit button):
```tsx
<AnalysisSection>
  <PersonaIndicator>
    Analyzing as: <PersonaBadge persona={selectedPersona} />
    <Link to={`/controls/${controlNumber}`}>Change persona</Link>
  </PersonaIndicator>
  
  <AnalyzeButton 
    onClick={requestAnalysis}
    disabled={isAnalyzing || !hasResponses}
  >
    {isAnalyzing ? (
      <><Spinner /> Analyzing...</>
    ) : (
      <><Sparkles /> Analyze with AI</>
    )}
  </AnalyzeButton>
  
  {!hasResponses && (
    <HelpText>Add questionnaire responses to enable AI analysis</HelpText>
  )}
</AnalysisSection>

{analysisResult && (
  <AnalysisResultsPanel>
    {/* Streaming content appears here */}
  </AnalysisResultsPanel>
)}
```

### 2. Streaming Analysis API
```typescript
// POST /api/controls/:controlNumber/analyze
// Uses Server-Sent Events for streaming

Request body: {
  persona: 'Auditor' | 'Advisor' | 'Analyst',
  include_history: boolean,  // include last 3 test results
  comments: string  // tester's comments from form
}

Response: Server-Sent Events stream
event: token
data: {"text": "Based on the evidence..."}

event: complete
data: {
  "assessment": "Overall assessment text...",
  "suggested_status": "Pass",
  "confidence": 0.85,
  "observations": ["...", "..."],
  "recommendations": ["...", "..."],
  "persona_specific": {
    // Varies by persona - see below
  }
}
```

### 3. Persona-Specific Output Structure

**Auditor persona_specific:**
```json
{
  "red_flags_triggered": ["Generic template detected", "No review evidence"],
  "nc_risk_areas": ["Documentation currency", "Stakeholder traceability"],
  "evidence_gaps": ["Missing dated review", "No version control"],
  "audit_opinion": "Partial conformity - documentation exists but lacks currency"
}
```

**Advisor persona_specific:**
```json
{
  "improvement_opportunities": [
    "Add version control to context document",
    "Include climate change considerations"
  ],
  "maturity_assessment": "Developing - processes exist but not consistently applied",
  "related_controls_to_review": ["RQ6", "5.1", "5.7"],
  "quick_wins": ["Update document date", "Add stakeholder sign-off"]
}
```

**Analyst persona_specific:**
```json
{
  "compliance_score": 67,
  "criteria_met": 4,
  "criteria_total": 6,
  "gap_analysis": [
    {"criterion": "Dated review", "status": "missing", "weight": "high"},
    {"criterion": "Version control", "status": "partial", "weight": "medium"}
  ],
  "suggested_kpis": [
    "Document review completion rate",
    "Days since last context update"
  ]
}
```

### 4. AI System Prompt Construction
```typescript
function buildAnalysisSystemPrompt(
  persona: Persona,
  control: Control,
  questions: OntologyQuestion[]
): string {
  const basePrompt = `You are an ISO 27001:2022 compliance ${persona.toLowerCase()} 
analyzing responses for control ${control.control_number}: ${control.name}.

Your task is to evaluate whether the responses demonstrate adequate implementation 
and suggest an appropriate test status.`;

  const personaPrompts = {
    Auditor: `
As an AUDITOR, your role is evidence verification. You must:
1. Verify each response against the expected evidence type
2. Check for RED FLAGS - explicitly call out any that apply:
${questions.map(q => `   - Q${q.question_id}: ${q.red_flags}`).join('\n')}
3. Reference NC (non-conformity) patterns when issues found
4. Be direct about documentation gaps
5. Consider if an external certification auditor would accept this evidence

Evaluation criteria per question:
${questions.map(q => `Q${q.question_id} - Auditor Focus: ${q.auditor_focus}`).join('\n')}

Your assessment should read like an audit observation.`,

    Advisor: `
As an ADVISOR, your role is improvement guidance. You must:
1. Compare responses to "what good looks like":
${questions.map(q => `   - Q${q.question_id}: ${q.what_good_looks_like}`).join('\n')}
2. Identify improvement opportunities constructively
3. Consider related controls for holistic recommendations: 
${[...new Set(questions.map(q => q.related_controls))].join(', ')}
4. Assess maturity level (Initial/Developing/Defined/Managed/Optimized)
5. Balance critique with recognition of strengths

Your assessment should read like a consulting recommendation.`,

    Analyst: `
As an ANALYST, your role is quantitative assessment. You must:
1. Score each response against its criteria (met/partial/not met)
2. Calculate overall compliance percentage
3. Quantify gaps with specific counts and percentages
4. Identify measurable improvement areas
5. Suggest KPIs for ongoing monitoring

Evaluation criteria counts:
${questions.map(q => {
  const criteria = q.what_good_looks_like.split(' and ').length;
  return `Q${q.question_id}: ${criteria} criteria`;
}).join('\n')}

Your assessment should read like a data-driven report.`
  };

  return `${basePrompt}\n${personaPrompts[persona]}`;
}
```

### 5. User Message Construction
```typescript
function buildAnalysisUserMessage(
  questions: OntologyQuestion[],
  responses: QuestionResponse[],
  comments: string,
  previousTests?: TestRun[]
): string {
  let message = `## Questionnaire Responses\n\n`;
  
  for (const q of questions) {
    const response = responses.find(r => r.question_id === q.question_id);
    message += `### Q${q.question_id}: ${q.question}\n`;
    message += `Expected evidence: ${q.evidence_type}\n`;
    message += `Response: ${response?.response_text || '(No response provided)'}\n`;
    if (response?.evidence_references?.length) {
      message += `Evidence cited: ${response.evidence_references.join(', ')}\n`;
    }
    message += `\n`;
  }
  
  message += `## Tester Comments\n${comments || '(None provided)'}\n\n`;
  
  if (previousTests?.length) {
    message += `## Previous Test History\n`;
    for (const test of previousTests.slice(0, 3)) {
      message += `- ${test.test_date}: ${test.status}`;
      if (test.comments) message += ` - "${test.comments}"`;
      message += `\n`;
    }
  }
  
  message += `\n## Required Output Format
Provide your analysis as JSON:
{
  "assessment": "2-3 sentence overall assessment",
  "suggested_status": "Pass|Fail|ContinualImprovement|NotAttempted",
  "confidence": 0.0-1.0,
  "observations": ["observation 1", "observation 2"],
  "recommendations": ["recommendation 1", "recommendation 2"],
  "persona_specific": { /* persona-specific fields */ }
}`;

  return message;
}
```

### 6. Streaming Implementation
```typescript
// Backend: routes/analysis.ts
import Anthropic from '@anthropic-ai/sdk';

app.post('/api/controls/:controlNumber/analyze', async (req, res) => {
  const { persona, include_history, comments } = req.body;
  
  // Set up SSE
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');
  
  const anthropic = new Anthropic();
  
  const stream = await anthropic.messages.stream({
    model: 'claude-sonnet-4-20250514',
    max_tokens: 2000,
    system: buildAnalysisSystemPrompt(persona, control, questions),
    messages: [{ 
      role: 'user', 
      content: buildAnalysisUserMessage(questions, responses, comments, history)
    }]
  });
  
  for await (const event of stream) {
    if (event.type === 'content_block_delta') {
      res.write(`event: token\ndata: ${JSON.stringify({ text: event.delta.text })}\n\n`);
    }
  }
  
  const finalMessage = await stream.finalMessage();
  const fullText = finalMessage.content[0].text;
  
  // Parse JSON from response
  const analysis = parseAnalysisJSON(fullText);
  
  // Log AI interaction
  await logAIInteraction({
    user_id: 1,
    interaction_type: 'test_analysis',
    control_id: control.id,
    input_summary: `${persona} analysis for ${control.control_number}`,
    output_summary: `Status: ${analysis.suggested_status}, Confidence: ${analysis.confidence}`,
    model_used: 'claude-sonnet-4-20250514',
    tokens_used: finalMessage.usage.input_tokens + finalMessage.usage.output_tokens
  });
  
  res.write(`event: complete\ndata: ${JSON.stringify(analysis)}\n\n`);
  res.end();
});
```

### 7. Frontend Streaming Handler
```typescript
// hooks/useAIAnalysis.ts
export function useAIAnalysis(controlNumber: string) {
  const [isAnalyzing, setIsAnalyzing] = useState(false);
  const [streamedText, setStreamedText] = useState('');
  const [result, setResult] = useState<AnalysisResult | null>(null);
  const [error, setError] = useState<string | null>(null);
  
  const analyze = async (persona: Persona, comments: string) => {
    setIsAnalyzing(true);
    setStreamedText('');
    setResult(null);
    setError(null);
    
    try {
      const response = await fetch(`/api/controls/${controlNumber}/analyze`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ persona, include_history: true, comments })
      });
      
      const reader = response.body?.getReader();
      const decoder = new TextDecoder();
      
      while (true) {
        const { done, value } = await reader!.read();
        if (done) break;
        
        const chunk = decoder.decode(value);
        const lines = chunk.split('\n');
        
        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = JSON.parse(line.slice(6));
            if (data.text) {
              setStreamedText(prev => prev + data.text);
            } else if (data.suggested_status) {
              setResult(data);
            }
          }
        }
      }
    } catch (err) {
      setError(err.message);
    } finally {
      setIsAnalyzing(false);
    }
  };
  
  return { analyze, isAnalyzing, streamedText, result, error };
}
```

### 8. Analysis Results Display
```tsx
<AnalysisResultsPanel>
  {/* Streaming text display */}
  {isAnalyzing && (
    <StreamingText>{streamedText}</StreamingText>
  )}
  
  {/* Final results */}
  {result && (
    <>
      <StatusSuggestion>
        <SuggestedBadge status={result.suggested_status} />
        <ConfidenceMeter value={result.confidence} />
        <span>{Math.round(result.confidence * 100)}% confidence</span>
      </StatusSuggestion>
      
      <Assessment>{result.assessment}</Assessment>
      
      <ObservationsList>
        <h4>Observations</h4>
        <ul>
          {result.observations.map((obs, i) => <li key={i}>{obs}</li>)}
        </ul>
      </ObservationsList>
      
      {/* Persona-specific sections */}
      {persona === 'Auditor' && result.persona_specific.red_flags_triggered?.length > 0 && (
        <RedFlagsAlert>
          <AlertTriangle className="text-red-500" />
          <h4>Red Flags Identified</h4>
          <ul>
            {result.persona_specific.red_flags_triggered.map((flag, i) => (
              <li key={i}>{flag}</li>
            ))}
          </ul>
        </RedFlagsAlert>
      )}
      
      {persona === 'Advisor' && (
        <ImprovementSection>
          <h4>Improvement Opportunities</h4>
          <ul>
            {result.persona_specific.improvement_opportunities?.map((opp, i) => (
              <li key={i}>{opp}</li>
            ))}
          </ul>
          <MaturityBadge level={result.persona_specific.maturity_assessment} />
        </ImprovementSection>
      )}
      
      {persona === 'Analyst' && (
        <MetricsSection>
          <ComplianceScore value={result.persona_specific.compliance_score} />
          <span>
            {result.persona_specific.criteria_met} of {result.persona_specific.criteria_total} criteria met
          </span>
          <h4>Suggested KPIs</h4>
          <ul>
            {result.persona_specific.suggested_kpis?.map((kpi, i) => (
              <li key={i}>{kpi}</li>
            ))}
          </ul>
        </MetricsSection>
      )}
      
      <RecommendationsList>
        <h4>Recommendations</h4>
        <ul>
          {result.recommendations.map((rec, i) => <li key={i}>{rec}</li>)}
        </ul>
      </RecommendationsList>
      
      <ApplyButton onClick={() => setStatus(result.suggested_status)}>
        Apply Suggested Status: {result.suggested_status}
      </ApplyButton>
    </>
  )}
</AnalysisResultsPanel>
```

### 9. Test Run Record Enhancement

When submitting the test, include AI analysis:
```typescript
// Existing test_runs columns used:
// - ai_analysis: full AI response text
// - ai_suggested_status: AI's suggestion
// - ai_confidence: confidence score
// - ai_context_scope: 'current_only' | 'last_3' | 'all_history'

// Add to test run creation:
const testRun = await db.insert(testRuns).values({
  organisation_control_id: orgControl.id,
  tester_user_id: 1,
  status: selectedStatus,  // Human's final decision
  comments: comments,
  ai_analysis: JSON.stringify(analysisResult),
  ai_suggested_status: analysisResult?.suggested_status,
  ai_confidence: analysisResult?.confidence,
  ai_context_scope: includeHistory ? 'last_3' : 'current_only'
});
```

### 10. Error Handling
```tsx
{error && (
  <ErrorAlert>
    {error.includes('API_KEY') ? (
      <>
        AI analysis requires an API key.
        <Link to="/settings">Configure in Settings â†’</Link>
      </>
    ) : error.includes('rate') ? (
      <>Rate limit reached. Please wait a moment and try again.</>
    ) : (
      <>Analysis failed: {error}</>
    )}
  </ErrorAlert>
)}
```

[ACCEPTANCE CRITERIA]
- [ ] "Analyze with AI" button appears on test recording form
- [ ] Analysis streams in real-time
- [ ] Persona affects AI voice and focus areas
- [ ] Auditor analysis shows red flags and NC risks
- [ ] Advisor analysis shows improvements and maturity
- [ ] Analyst analysis shows scores and KPIs
- [ ] Suggested status displayed with confidence
- [ ] User can apply or override suggested status
- [ ] Both human status and AI analysis stored in test_run
- [ ] AI interaction logged to ai_interactions table
- [ ] Works gracefully when no responses provided (suggests NotAttempted)
- [ ] Error handling for API failures

**Create checkpoint: "Persona AI Analysis"**